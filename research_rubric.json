{
  "version": "1.0",
  "created_at": "2026-02-25T12:00:00Z",
  "updated_at": "2026-02-25T10:49:37.323617+00:00",
  "current_agent": "researcher",
  "agent_status": {
    "orchestrator": {
      "status": "completed",
      "started_at": "2026-02-25T12:00:00Z",
      "completed_at": "2026-02-25T10:49:34.970469+00:00",
      "error": null
    },
    "researcher": {
      "status": "in_progress",
      "started_at": "2026-02-25T10:49:37.323592+00:00",
      "completed_at": null,
      "error": null
    },
    "writer": {
      "status": "pending",
      "started_at": null,
      "completed_at": null,
      "error": null
    },
    "reviewer": {
      "status": "pending",
      "started_at": null,
      "completed_at": null,
      "error": null
    }
  },
  "phases": [
    {
      "id": "phase_1",
      "name": "Problem Analysis & Literature Review",
      "order": 1,
      "items": [
        {
          "id": "item_001",
          "description": "Analyze repository structure and define project scaffold",
          "acceptance_criteria": "Create a MODULE_MAP.md listing all planned directories (src/, data/, benchmarks/, models/, scripts/, tests/) with purpose descriptions. Create a requirements.txt or pyproject.toml with core dependencies (osmnx, networkx, numpy, torch, matplotlib). All directories exist on disk.",
          "status": "completed",
          "notes": "Created MODULE_MAP.md, requirements.txt, and all directories (src/, src/models/, data/, benchmarks/, models/, scripts/, tests/, docs/, results/, figures/). __init__.py files added for Python packages.",
          "error": null
        },
        {
          "id": "item_002",
          "description": "Literature review: classical and modern TSP/ATSP solvers",
          "acceptance_criteria": "Produce a document (docs/lit_review_classical.md) covering at minimum: Concorde (Applegate et al. 2006), LKH/LKH-3 (Helsgott 2000, 2017), OR-Tools (Google), VROOM, and the Korea 81,998-bar OSRM tour (Cook & Helsgott 2024-2025). Each entry must include: key algorithm, complexity, strengths on road networks, and limitations. At least 8 distinct papers or technical reports cited.",
          "status": "completed",
          "notes": "Comprehensive review covering 8 solvers/methods: Concorde, LKH/LKH-3, OR-Tools, VROOM, Korea 81998-bar tour, Christofides, Held-Karp, OSRM. 12 distinct references cited. Each entry includes algorithm, complexity, strengths, and limitations.",
          "error": null
        },
        {
          "id": "item_003",
          "description": "Literature review: learned and hybrid TSP heuristics",
          "acceptance_criteria": "Produce a document (docs/lit_review_learned.md) covering at minimum: NeuroLKH (Xin et al. NeurIPS 2021), VSR-LKH (Zheng et al. 2022), Attention Model (Kool et al. 2019), POMO (Kwon et al. 2020), GREAT architecture (2024), Embed-LKH (2025), MAB-backbone-LKH (Journal of Heuristics 2025), UNiCS (Liu et al. 2025), DualOpt, and H-TSP. Each entry summarizes the method, training procedure, and reported results. At least 10 distinct papers cited.",
          "status": "pending",
          "notes": null,
          "error": null
        },
        {
          "id": "item_004",
          "description": "Survey real-world ATSP datasets, OSRM APIs, and OSM data pipelines",
          "acceptance_criteria": "Produce a document (docs/data_survey.md) covering: TSPLIB95 ATSP instances, DIMACS ATSP challenge, Waterloo TSP data, Ascheuer TSPTW instances, OSRM Table/Trip API capabilities, osmnx library for graph extraction, and at least 2 real-world road network sources (e.g., OpenStreetMap extracts for specific metro areas). Document data formats, sizes, and accessibility.",
          "status": "pending",
          "notes": null,
          "error": null
        },
        {
          "id": "item_005",
          "description": "Create and populate sources.bib with BibTeX entries for all consulted sources",
          "acceptance_criteria": "A file sources.bib exists at repo root with at least 15 complete BibTeX entries covering classical solvers, learned heuristics, ATSP benchmarks, and OSRM/OSM tooling. Each entry must have author, title, year, and venue/url fields. Entries must be parseable by standard BibTeX tools.",
          "status": "pending",
          "notes": null,
          "error": null
        },
        {
          "id": "item_006",
          "description": "Formalize research problem statement and hypotheses",
          "acceptance_criteria": "Produce a document (docs/problem_statement.md) that: (1) formally defines the Asymmetric TSP on road networks with time-dependent edge weights; (2) identifies the specific gap \u2014 LKH/Concorde assume metric/Euclidean distances while real road networks have asymmetric costs, one-way streets, turn penalties, and traffic; (3) states at least 3 falsifiable hypotheses (e.g., 'A GNN-based candidate set trained on OSRM features will reduce LKH tour cost by >= 0.5% on 200-stop road-network instances compared to default LKH alpha-nearness candidates'); (4) defines success criteria for the project.",
          "status": "pending",
          "notes": null,
          "error": null
        }
      ]
    },
    {
      "id": "phase_2",
      "name": "Baseline Implementation & Metrics",
      "order": 2,
      "items": [
        {
          "id": "item_007",
          "description": "Build OSM/OSRM data pipeline to generate asymmetric distance/duration matrices",
          "acceptance_criteria": "A Python module (src/data_pipeline.py or src/osrm_client.py) that: (1) accepts a bounding box or city name and a list of coordinates; (2) queries OSRM Table API to produce an asymmetric NxN duration matrix and an NxN distance matrix; (3) caches results to disk in a standard format (JSON or NumPy .npz); (4) alternatively loads from osmnx graph for offline use. Module includes at least 3 unit tests verifying matrix asymmetry and correct dimensions. Tested on at least one real city (e.g., Manhattan).",
          "status": "pending",
          "notes": null,
          "error": null
        },
        {
          "id": "item_008",
          "description": "Generate benchmark instance suite from real road networks",
          "acceptance_criteria": "Create at least 15 benchmark instances across 3 scales: small (50 stops), medium (200 stops), large (1000 stops), drawn from at least 3 distinct metro areas (e.g., Manhattan NYC, central London, downtown Tokyo or Berlin). Each instance stored in benchmarks/ directory with: coordinates, OSRM asymmetric duration matrix, and metadata (city, date generated, OSRM version). Include a benchmarks/README.md describing each instance.",
          "status": "pending",
          "notes": null,
          "error": null
        },
        {
          "id": "item_009",
          "description": "Implement baseline solvers: LKH-3, OR-Tools, nearest-neighbor, OSRM Trip",
          "acceptance_criteria": "A Python module (src/baselines.py) implementing a uniform solver interface with at least 4 solvers: (1) LKH-3 wrapper (via subprocess or pylkh); (2) Google OR-Tools routing solver; (3) greedy nearest-neighbor heuristic for ATSP; (4) OSRM Trip API (farthest-insertion). Each solver accepts an asymmetric cost matrix and returns a tour (ordered list of node indices) and total cost. All 4 solvers produce valid tours on a 50-stop test instance.",
          "status": "pending",
          "notes": null,
          "error": null
        },
        {
          "id": "item_010",
          "description": "Define evaluation metrics and benchmarking harness",
          "acceptance_criteria": "A Python module (src/metrics.py) and a runner script (scripts/run_benchmarks.py) that: (1) computes tour cost on the asymmetric matrix; (2) computes optimality gap relative to best-known solution; (3) records wall-clock time and peak memory usage; (4) outputs results to a CSV or JSON file with columns: instance_id, solver, tour_cost, gap_pct, time_sec, memory_mb. Harness supports running all solvers on all instances with configurable time limits and random seeds.",
          "status": "pending",
          "notes": null,
          "error": null
        },
        {
          "id": "item_011",
          "description": "Run baseline benchmarks and record reference results",
          "acceptance_criteria": "Execute all 4 baseline solvers on all 15+ benchmark instances with 5 random seeds each. Store results in results/baseline_results.csv. Report: (1) mean and std tour cost per solver per scale; (2) mean optimality gap vs. best solution found across all solvers; (3) mean computation time. LKH-3 must produce the best mean tour cost on >= 80% of instances (confirming it as the target to beat). Document findings in results/baseline_analysis.md.",
          "status": "pending",
          "notes": null,
          "error": null
        }
      ]
    },
    {
      "id": "phase_3",
      "name": "Core Research & Novel Approaches",
      "order": 3,
      "items": [
        {
          "id": "item_012",
          "description": "Design and implement asymmetry-aware edge-scoring GNN for road-network graphs",
          "acceptance_criteria": "A PyTorch module (src/models/edge_scorer.py) implementing a directed graph neural network that: (1) takes as input node features (lat, lon, degree, betweenness centrality) and directed edge features (duration, distance, speed, asymmetry ratio d(i,j)/d(j,i)); (2) outputs per-edge scores predicting probability of edge being in optimal tour; (3) uses at least 3 message-passing layers with attention or gating; (4) architecture documented in docs/model_architecture.md. Model compiles and runs forward pass on a 50-node test graph without errors.",
          "status": "pending",
          "notes": null,
          "error": null
        },
        {
          "id": "item_013",
          "description": "Train edge-scoring model on road-network TSP instances with supervised labels",
          "acceptance_criteria": "A training script (scripts/train_edge_scorer.py) that: (1) generates training data by solving 500+ small instances (50-100 stops) with LKH-3 to get near-optimal tours as labels; (2) trains the GNN with binary cross-entropy on edge inclusion; (3) achieves >= 85% precision and >= 70% recall on held-out validation set for predicting tour edges; (4) saves trained model checkpoint to models/. Training logs recorded in results/training_log.csv.",
          "status": "pending",
          "notes": null,
          "error": null
        },
        {
          "id": "item_014",
          "description": "Implement learned candidate set generation for LKH on road-network graphs",
          "acceptance_criteria": "A module (src/learned_candidates.py) that: (1) uses the trained edge-scoring GNN to rank edges by predicted tour membership probability; (2) constructs a candidate set of top-k edges per node (k=5 to 10) for LKH; (3) integrates with LKH-3 by writing a candidate file in LKH's expected format; (4) on a 200-stop test instance, the learned candidate set contains >= 90% of edges in the LKH-found optimal tour (candidate set recall). Compare candidate set quality against LKH's default alpha-nearness candidates.",
          "status": "pending",
          "notes": null,
          "error": null
        },
        {
          "id": "item_015",
          "description": "Develop traffic-aware and time-dependent cost model",
          "acceptance_criteria": "A module (src/traffic_model.py) that: (1) models time-dependent edge costs using a piecewise-linear speed profile per edge (at minimum: peak morning, off-peak, peak evening, night); (2) computes departure-time-dependent tour costs correctly (i.e., arrival time at each stop determines the cost of the next edge); (3) supports loading real or synthetic traffic multipliers; (4) includes a utility to generate synthetic but realistic traffic patterns based on road type (highway, arterial, local). Verified with a test showing tour cost varies by >= 10% between peak and off-peak departure times on a 50-stop instance.",
          "status": "pending",
          "notes": null,
          "error": null
        },
        {
          "id": "item_016",
          "description": "Implement RL-guided local search move selection for ATSP improvement",
          "acceptance_criteria": "A module (src/local_search.py) implementing: (1) classical 2-opt, or-opt, and relocate moves for asymmetric tours; (2) an RL agent (Q-learning or policy gradient) that learns to select which move type and which nodes to apply it to, given current tour state features; (3) the RL agent is trained on at least 200 small instances (50-100 stops); (4) on a 200-stop validation instance, RL-guided local search finds a tour at least as good as random-restart 2-opt in <= 50% of the wall-clock time. Document the RL state/action/reward design in docs/rl_local_search.md.",
          "status": "pending",
          "notes": null,
          "error": null
        },
        {
          "id": "item_017",
          "description": "Build hybrid solver combining learned candidate generation + LKH local search",
          "acceptance_criteria": "A module (src/hybrid_solver.py) that: (1) uses the trained GNN edge scorer to generate candidate sets; (2) feeds those candidates to LKH-3 as the initial candidate set; (3) optionally applies the RL-guided local search as a post-processing step; (4) exposes a unified API matching the baseline solver interface (accepts cost matrix, returns tour and cost); (5) on a 200-stop test instance, produces a tour within 1% of LKH-3 default in <= 80% of LKH-3's computation time OR produces a strictly better tour given equal time. Document the integration approach.",
          "status": "pending",
          "notes": null,
          "error": null
        }
      ]
    },
    {
      "id": "phase_4",
      "name": "Experiments & Evaluation",
      "order": 4,
      "items": [
        {
          "id": "item_018",
          "description": "Full benchmark comparison: hybrid solver vs all baselines on all instance sets",
          "acceptance_criteria": "Run the hybrid solver and all 4 baselines on all 15+ benchmark instances with 10 random seeds each and 3 time limits (1s, 10s, 60s per instance). Store results in results/full_comparison.csv. The hybrid solver must achieve a lower mean tour cost than LKH-3 default on at least 60% of the 200-stop and 1000-stop instances under at least one time limit. Report mean gap improvement in percentage.",
          "status": "pending",
          "notes": null,
          "error": null
        },
        {
          "id": "item_019",
          "description": "Scalability study: measure performance from 50 to 5000 stops",
          "acceptance_criteria": "Generate additional instances at 500, 2000, and 5000 stops. Run hybrid solver and LKH-3 on all scales (50, 200, 500, 1000, 2000, 5000) with 5 seeds each. Record: tour cost, gap vs LKH-3, total time (including GNN inference + LKH search), and GNN inference time separately. Plot scaling curves (time vs. N, gap vs. N). The hybrid solver must remain competitive (gap < 2% vs LKH-3) at all scales up to at least 2000 stops. Store in results/scalability_results.csv.",
          "status": "pending",
          "notes": null,
          "error": null
        },
        {
          "id": "item_020",
          "description": "Ablation study: isolate contribution of each learned component",
          "acceptance_criteria": "Run 4 ablation configurations on all 200-stop instances with 10 seeds: (A) LKH-3 default only; (B) LKH-3 + learned candidates only; (C) LKH-3 + RL local search only; (D) full hybrid (learned candidates + RL local search). Report mean tour cost and time for each configuration. Determine which component contributes the most improvement. Store in results/ablation_results.csv. Write analysis in results/ablation_analysis.md explaining the contribution of each component with statistical support.",
          "status": "pending",
          "notes": null,
          "error": null
        },
        {
          "id": "item_021",
          "description": "Statistical significance testing and confidence intervals",
          "acceptance_criteria": "For the primary comparison (hybrid vs LKH-3 default), compute: (1) paired Wilcoxon signed-rank test p-values per instance scale; (2) 95% confidence intervals for the mean gap improvement; (3) effect size (Cohen's d). Results must demonstrate p < 0.05 for at least the 200-stop and 1000-stop instance sets, or honestly report failure to achieve significance. Store statistical analysis in results/statistical_tests.md with all test outputs.",
          "status": "pending",
          "notes": null,
          "error": null
        },
        {
          "id": "item_022",
          "description": "Compare results against published benchmarks from literature review",
          "acceptance_criteria": "Produce a comparison table (results/literature_comparison.md) placing our hybrid solver results alongside published results from at least 5 prior methods identified in Phase 1 (e.g., NeuroLKH, VSR-LKH, VROOM, attention model, OR-Tools). For each method, report: problem type (symmetric/asymmetric), instance sizes tested, reported gap vs optimal/LKH, and computation time. Discuss where our approach improves and where it falls short, citing sources.bib entries.",
          "status": "pending",
          "notes": null,
          "error": null
        }
      ]
    },
    {
      "id": "phase_5",
      "name": "Analysis & Documentation",
      "order": 5,
      "items": [
        {
          "id": "item_023",
          "description": "Write comprehensive research report with methodology and results",
          "acceptance_criteria": "Produce FINDINGS.md (>= 3000 words) with sections: Abstract, Introduction, Related Work (citing >= 10 entries from sources.bib), Problem Formulation, Methodology (data pipeline, GNN architecture, RL local search, hybrid integration), Experimental Setup, Results (with tables and figure references), Discussion, and Conclusion. All claims supported by experimental data from Phase 4.",
          "status": "pending",
          "notes": null,
          "error": null
        },
        {
          "id": "item_024",
          "description": "Generate publication-quality figures and summary results tables",
          "acceptance_criteria": "Produce at least 6 figures in results/figures/ (PNG and PDF): (1) bar chart of tour costs across all solvers at each scale; (2) scaling curve (time vs N) for all solvers; (3) gap improvement histogram for hybrid vs LKH-3; (4) ablation component contribution chart; (5) candidate set recall vs candidate set size plot; (6) traffic impact on tour cost across time-of-day. All figures have labeled axes, legends, and captions. Produce at least 2 LaTeX-formatted or markdown tables summarizing key numerical results.",
          "status": "pending",
          "notes": null,
          "error": null
        },
        {
          "id": "item_025",
          "description": "Create reproducibility package with dependencies, scripts, and documentation",
          "acceptance_criteria": "Repository root contains: (1) requirements.txt with pinned versions for all Python dependencies; (2) a Makefile or scripts/run_all.sh that reproduces the full pipeline (data generation -> training -> benchmarking -> figures) with a single command; (3) README.md updated with: project overview, installation instructions, usage examples, and description of all result files; (4) a DOCKER.md or Dockerfile for containerized reproduction (optional but preferred). A fresh clone + install + run produces the key result tables.",
          "status": "pending",
          "notes": null,
          "error": null
        },
        {
          "id": "item_026",
          "description": "Document limitations, failure modes, and future research directions",
          "acceptance_criteria": "Produce docs/limitations.md covering at minimum: (1) instances or scales where the hybrid solver underperforms LKH-3; (2) GNN generalization limitations (trained on city A, tested on city B); (3) traffic model simplifications vs real-time traffic; (4) computational overhead of GNN inference at scale; (5) at least 3 concrete future research directions (e.g., CVRPTW extension, real-time re-optimization, transfer learning across cities). Each limitation supported by experimental evidence from Phase 4.",
          "status": "pending",
          "notes": null,
          "error": null
        },
        {
          "id": "item_027",
          "description": "Final validation checklist: verify all acceptance criteria and artifacts",
          "acceptance_criteria": "Produce a CHECKLIST.md that enumerates all 27 rubric items and for each: (1) states whether the acceptance criteria is fully met (PASS/FAIL); (2) lists the specific file(s) or artifact(s) produced; (3) notes any partial completions. At least 24 of 27 items must be PASS. sources.bib must have >= 15 entries. All test scripts pass without errors. Repository is clean (no uncommitted changes, no broken imports).",
          "status": "pending",
          "notes": null,
          "error": null
        }
      ]
    }
  ],
  "summary": {
    "total_items": 27,
    "completed": 2,
    "in_progress": 0,
    "failed": 0,
    "pending": 25
  }
}