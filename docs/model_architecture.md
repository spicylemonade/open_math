# Edge-Scoring GNN Architecture for Road-Network ATSP

## Overview

A directed graph neural network that predicts the probability of each edge being in the optimal tour on road-network ATSP instances. The model takes node features (location, graph topology) and directed edge features (duration, distance, speed, asymmetry) as input, and outputs per-edge scores.

## Architecture

### Input Features

**Node features (4 dimensions):**
- Latitude (normalized)
- Longitude (normalized)
- Out-degree (fraction of reachable destinations within threshold)
- In-degree (fraction of reachable sources within threshold)

**Edge features (4 dimensions):**
- Duration (normalized by max duration)
- Distance (normalized by max distance)
- Speed proxy (inverse duration, normalized)
- Asymmetry ratio: c(i,j) / c(j,i) (clamped and normalized)

### Graph Construction

For each node, edges connect to its k=20 nearest neighbors by travel time. This sparsification:
- Reduces the quadratic edge count to O(nk)
- Retains the most relevant candidate edges for tour construction
- Is similar to LKH's candidate set concept

### Network Layers

**1. Input Embedding (2 layers each):**
- Node: Linear(4 → 64) → ReLU → Linear(64 → 64)
- Edge: Linear(4 → 64) → ReLU → Linear(64 → 64)

**2. Directed Edge Attention Layers (×3):**
Each layer performs:

a. **Multi-head attention** (4 heads, 16 dims each):
   - Query from destination node: Q = W_q · h_dst
   - Key from source node + edge: K = W_k · h_src + W_e · e_ij
   - Attention weight: α = softmax(Q · K / √d)

b. **Message aggregation**:
   - Value from source: V = W_v · h_src
   - Aggregated message: m_j = Σ_i α_ij · V_i

c. **Gated node update**:
   - Gate: g = σ(W_gate · [m_j; W_o · h_j])
   - Update: h_j' = g · W_o(m_j) + (1-g) · h_j
   - Residual + LayerNorm

d. **Edge feature update**:
   - e_ij' = MLP([h_i'; h_j'; e_ij]) + e_ij
   - LayerNorm

**3. Edge Classification Head:**
- Linear(64 → 64) → ReLU → Dropout(0.1) → Linear(64 → 1) → Sigmoid

### Key Design Choices

1. **Directed attention**: Query from destination, key from source preserves edge directionality. Critical for ATSP where c(i,j) ≠ c(j,i).

2. **Edge feature conditioning**: Edge features modulate attention weights, allowing the model to learn that high-asymmetry or high-speed edges have different importance.

3. **Gated aggregation**: The sigmoid gate controls how much the node updates from neighbors vs. retains its own state. Prevents over-smoothing.

4. **Edge updates**: Edge features are updated using both endpoint embeddings, enabling multi-hop information propagation through edges.

5. **k-NN sparsification**: Limits computation to O(nk) edges while retaining high candidate recall.

## Model Statistics

- Parameters: ~150K
- Layers: 3 message-passing + 2 embedding + 1 classifier
- Attention heads: 4
- Hidden dimension: 64

## Training

- Loss: Binary cross-entropy on edge labels (1 = edge in optimal tour)
- Optimizer: Adam with learning rate scheduling
- Labels: Generated by solving small instances with the best available solver
- Class imbalance: Handled via positive weight in BCE loss (since only n out of nk edges are in the tour)
